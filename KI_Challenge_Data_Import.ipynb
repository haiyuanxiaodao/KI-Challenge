{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KI Challenge \n",
    "\n",
    "# Dokumentenklassifizierung\n",
    "\n",
    "## Lena Hinkel, lena.hinkel@gmx.de, 1981095\n",
    "## Lars Böcking, boecking.lars@googlemail.com, 2000264\n",
    "## Mariana Zehender, marianazehender@aol.com, 2056046\n",
    "## Hongchen Ji, hongchenji@gmail.com, 1962863\n",
    "## Xiaoyu Yang, yxy6677@gmail.com, 2132536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6c65ae1bdca9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import random \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import csv\n",
    "from spacy.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data - Negativ und Positiv gekennzeichnete Rezensionen für das Trainieren des Algorithmus \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingSets():\n",
    "    with open(\"pos.txt\") as file:\n",
    "        dataPos = file.readlines()\n",
    "    with open(\"neg.txt\") as file: \n",
    "        dataNeg = file.readlines()\n",
    "        \n",
    "    return dataPos, dataNeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEvaluationsSet():\n",
    "    with open(\"evaluation.txt\") as file:\n",
    "        evaluation = file.readlines()\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbereitungen für NLP (Natural Language Processing) = Stemming and Lemmatiization\n",
    "\n",
    "### Erstmal Preprocessing\n",
    "#### (Aufbereiten und Analysieren von unstruktuierten Texten - Siehe Datensätze dataPos und dataNeg)\n",
    "\n",
    "Lemmatisierung: Reduziere ein Wort – mittels eines festgelegten Korpus – auf den Wortstamm.\n",
    "traf, trifft, treffen, treffe à treffen (aus Übung 1)\n",
    "\n",
    "Wortfilterung (Stopwords removal: Lösche Stoppwörter, z.B. der, die, das, ein, über Stopplisten im Internet (aus Übung 1) - Lösche Zahlen und Satzzeichen --> Ersetze diese Zeichen mit Leerzeichen\n",
    "\n",
    "Synonyme: Ersetze Wörter mit derselben Bedeutung mit einem identischen Synonym. Meist mittels lexikalischer Datenbank, wie bspw. Wordnet (aus Übung 1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "\n",
    "def datenAufbereitung(datensatz): \n",
    "    # 1 Stopwords Removal\n",
    "    documentLen = len(datensatz)\n",
    "\n",
    "    intab = \"!\\\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~1234567890\" \n",
    "    outtab = \"\"\n",
    "    for i in range(len(intab)):\n",
    "        outtab += \" \"\n",
    "\n",
    "    for i in range(documentLen): \n",
    "        datensatz[i] = datensatz[i].translate(str.maketrans(intab, outtab))\n",
    "        datensatz[i]=datensatz[i].lower()\n",
    "\n",
    "    sp = spacy.load('en_core_web_sm', disable=[\"parser\", \"ner\", \"tagger\"])\n",
    "    token = Tokenizer(sp.vocab)\n",
    "    \n",
    "    for i in range(documentLen):\n",
    "        datensatz[i] = token(datensatz[i])\n",
    "        cache = \"\"\n",
    "        for words in datensatz[i]:\n",
    "             cache += \" \" +words.lemma_            \n",
    "        datensatz[i] = cache\n",
    "\n",
    "    return datensatz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aussortierter Code\n",
    "\n",
    "print(\"erster Versuch lemmatisierung\")\n",
    "\n",
    "\"\"\"# 2 Lemmatisierung \n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "print(dataPos[0])\n",
    "\n",
    "for i in range(documentLen):\n",
    "    cache = \"\"\n",
    "    for word in dataPos[i].split():\n",
    "        cache += \" \" +lemmatizer.lemmatize(word)\n",
    "    dataPos[i] = cache\n",
    "    \n",
    "print(dataPos[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineDataSets(set1, set2):\n",
    "    dataCombined = []\n",
    "\n",
    "    for rec in set1:\n",
    "        dataCombined.append(str(rec))\n",
    "    for rec in set2:\n",
    "        dataCombined.append(str(rec))\n",
    "        \n",
    "    return dataCombined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vektorisierung\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer #tfidf steht für Term Frequency-Inverse Document Frequency\n",
    "\n",
    "def vectorizer(dataset):\n",
    "    \n",
    "    corpus = dataset\n",
    "\n",
    "    cv = CountVectorizer()\n",
    "    word_count_vector = cv.fit_transform(corpus)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    matrix = vectorizer.fit_transform(corpus)\n",
    "    tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "    tfidf_transformer.fit(word_count_vector)\n",
    "    \n",
    "    df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n",
    "    df_idf.sort_values(by=['idf_weights'])\n",
    "    \n",
    "    # count matrix\n",
    "    count_vector= cv.transform(dataset)\n",
    " \n",
    "    # tf-idf scores\n",
    "    tf_idf_vector= tfidf_transformer.transform(count_vector)\n",
    "    \n",
    "    return cv, tfidf_transformer, tf_idf_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in \"tf_idf_vector\" haben wir alle 5007 reviews in Form der tf_idf abgespeichert. Das bedeutet, dass wir jedes Review auf einen Vektor der Größe 10158 abbilden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def splitDataSet(tf_idf_vector):\n",
    "    \n",
    "    x_train = tf_idf_vector.toarray()\n",
    "    y_train = []\n",
    "\n",
    "    for i in range(len(dataPos)):\n",
    "        y_train.append(1)\n",
    "    for i in range(len(dataNeg)):\n",
    "        y_train.append(0)\n",
    "        \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size=0.33, random_state=1)\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, RidgeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def trainClassifier(X_train, X_test, Y_train, Y_test):\n",
    "    \n",
    "    #clf = PassiveAggressiveClassifier(max_iter=1000, random_state=0,tol=1e-3)\n",
    "    #clf = Perceptron(tol=1e-3, random_state=0)\n",
    "    #clf = RidgeClassifier().fit(X_train, Y_train)\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    predictions = clf.predict(X_test)\n",
    "    print(f'Accuracy: {round(accuracy_score(Y_test, predictions),4)}')\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicEvaluation(csf, cv, evaluation, tf_idf_vector_eval):\n",
    "    # count matrix\n",
    "    count_vector= cv.transform(evaluation)\n",
    " \n",
    "    # tf-idf scores\n",
    "    tf_idf_vector_eval = tfidf_transformer.transform(count_vector)\n",
    "    results = csf.predict(tf_idf_vector_eval)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "          \n",
    "# Aussortierter Code\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "print(len(X_train))\n",
    "input_shape = X_train[0].shape\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10, input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('softmax'))\n",
    "          \n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"Adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "              batch_size=1,\n",
    "              epochs=10,\n",
    "              validation_data=(X_test, Y_test),\n",
    "              shuffle=True)\n",
    "    \n",
    "scores = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePredictions(results):\n",
    "    with open('predictionsEvaluation.csv', mode='w+', newline='') as pred_file:\n",
    "        pred_file= csv.writer(pred_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        i = 1\n",
    "        pred_file.writerow([\"Evaluationsdatensatz\", \"Prediction\"])\n",
    "        for pred in results:\n",
    "            pred_file.writerow([i, pred])\n",
    "            i+= 1\n",
    "    return pred_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lars/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_csv.writer at 0x1a26f68ef0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPos, dataNeg= getTrainingSets()\n",
    "\n",
    "dataPosBereinigt = datenAufbereitung(dataPos)\n",
    "dataNegBereinigt = datenAufbereitung(dataNeg)\n",
    "\n",
    "trainSet = combineDataSets(dataPosBereinigt, dataNegBereinigt)\n",
    "\n",
    "cv, tfidf_transformer, tf_idf_vector = vectorizer(trainSet)\n",
    "X_train, X_test, Y_train, Y_test = splitDataSet(tf_idf_vector)\n",
    "csf = trainClassifier(X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "evaluation = getEvaluationsSet()\n",
    "evaluation = datenAufbereitung(evaluation)\n",
    "\n",
    "predictions = predicEvaluation(csf, cv, evaluation, tfidf_transformer)\n",
    "\n",
    "savePredictions(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
